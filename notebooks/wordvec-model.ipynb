{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we demonstrate the use of **Word Embeddings (Word2Vec)** weighting technique into Information Retrieval to make trace link recovery between Use Cases and Bug Reports.\n",
    "\n",
    "We model our study as follows:\n",
    "\n",
    "* Each bug report title, summary and description compose a single query.\n",
    "* We use each use case content as an entire document that must be returned to the query made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support, pairwise_distances, pairwise\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "from enum import Enum\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import plots\n",
    "from utils import oracle_loader as ol\n",
    "from utils import jedit_dataset as jd\n",
    "from utils import tokenizers as tok\n",
    "from utils import aux_functions\n",
    "from utils import model_evaluator as m_eval\n",
    "from utils import generic_model as g_model\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df = jd.read_trace_df()\n",
    "artfs_desc_df = jd.read_artfs_desc_df()\n",
    "\n",
    "use_cases_df = artfs_desc_df[artfs_desc_df.artf_description.str.contains('Use Case ID')]\n",
    "bug_reports_df = artfs_desc_df[artfs_desc_df.artf_description.str.contains('Bug Number')]\n",
    "\n",
    "corpus = use_cases_df.artf_description\n",
    "query = bug_reports_df.artf_description\n",
    "\n",
    "use_cases_names = use_cases_df.artf_name\n",
    "bug_reports_names = bug_reports_df.artf_name\n",
    "\n",
    "orc = ol.OracleLoader(use_cases_names, bug_reports_names)\n",
    "orc.load(trace_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordVec Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVec_Model_Hyperp(Enum):\n",
    "    NAME = 'wordvec__name'\n",
    "    TOP = 'wordvec__top'\n",
    "    TOKENIZER = 'wordvec__tokenizer'\n",
    "    SIM_MEASURE_MIN_THRESHOLD = 'wordvec__sim_measure_min_threshold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Functions / Similarity Measures Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityMeasure(Enum):\n",
    "    COSINE = 'cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "params_dict = {\n",
    "    'wordvec__sim_measure_min_threshold' : ('cosine',.9),\n",
    "    'wordvec__name' : 'WordVec',\n",
    "    'wordvec__top' : 3\n",
    "    'wordvec_tokenizer' : WordNetBased_LemmaTokenizer()\n",
    "}\n",
    "\"\"\"\n",
    "class WordVec_BasedModel(g_model.GenericModel):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._nlp_model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.set_basic_params(**kwargs)\n",
    "        self.set_nlp_model()\n",
    "    \n",
    "    def set_name(self, name):\n",
    "        super().set_name(name)\n",
    "    \n",
    "    def set_basic_params(self, **kwargs):\n",
    "        super().set_name('WordVec' if WordVec_Model_Hyperp.NAME.value not in kwargs.keys() else kwargs[WordVec_Model_Hyperp.NAME.value])\n",
    "        super().set_sim_measure_min_threshold((SimilarityMeasure.COSINE.value,.80) if WordVec_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value not in kwargs.keys() else kwargs[WordVec_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value])\n",
    "        super().set_top(3 if WordVec_Model_Hyperp.TOP.value not in kwargs.keys() else kwargs[WordVec_Model_Hyperp.TOP.value])\n",
    "        super().set_model_gen_name('wordvector')\n",
    "        \n",
    "        self.tokenizer = tok.WordNetBased_LemmaTokenizer() if WordVec_Model_Hyperp.TOKENIZER.value not in kwargs.keys() else kwargs[WordVec_Model_Hyperp.TOKENIZER.value]\n",
    "        \n",
    "    \n",
    "    def set_nlp_model(self):\n",
    "        \"\"\"\n",
    "            WordVec based on GloVe 1.1M keys x 300 dim\n",
    "            300-dimensional word vectors trained on Common Crawl with GloVe.\n",
    "        \"\"\"\n",
    "        self._nlp_model = spacy.load('en_vectors_web_lg')\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        \"\"\"to pickle object serialization/deserialization\"\"\"\n",
    "        d = dict(self.__dict__)\n",
    "        del d['_nlp_model']\n",
    "        return d\n",
    "    \n",
    "    def __setstate__(self, d):\n",
    "        \"\"\"to pickle object serialization/deserialization\"\"\"\n",
    "        self.__dict__.update(d)\n",
    "    \n",
    "    def recover_links(self, corpus, query, use_cases_names, bug_reports_names):\n",
    "        return self._recover_links_cosine(corpus, query, use_cases_names, bug_reports_names)\n",
    "    \n",
    "    def _recover_links_cosine(self, corpus, query, use_cases_names, bug_reports_names):\n",
    "        list_corpus_tokens = [self.tokenizer.__call__(doc) for doc in corpus]\n",
    "        list_query_tokens = [self.tokenizer.__call__(doc) for doc in query]\n",
    "        \n",
    "        corpus = [' '.join(tok_list) for tok_list in list_corpus_tokens]\n",
    "        query = [' '.join(tok_list) for tok_list in list_query_tokens]\n",
    "        \n",
    "        self._sim_matrix = pd.DataFrame(index = use_cases_names, \n",
    "                                           columns = bug_reports_names,\n",
    "                                           data=np.zeros(shape=(len(use_cases_names), len(bug_reports_names)),dtype='float64'))\n",
    "        \n",
    "        for bug_id, bug_desc in zip(bug_reports_names, query):\n",
    "            for uc_id, uc_desc in zip(use_cases_names, corpus):\n",
    "                doc1 = self._nlp_model(bug_desc)\n",
    "                doc2 = self._nlp_model(uc_desc)\n",
    "                self._sim_matrix.at[uc_id, bug_id] = doc1.similarity(doc2)  # cosine similarity is default\n",
    "        \n",
    "        self._sim_matrix = pd.DataFrame(self._sim_matrix, index=use_cases_names, columns=bug_reports_names)\n",
    "        super()._fillUp_traceLinksDf(use_cases_names, bug_reports_names, self._sim_matrix)        \n",
    "    \n",
    "    def model_setup(self):\n",
    "        return {\"Setup\" : \n",
    "                  [\n",
    "                      {\"Name\" : super().get_name()},\n",
    "                      {\"Similarity Measure and Minimum Threshold\" : super().get_sim_measure_min_threshold()},\n",
    "                      {\"Top Value\" : super().get_top_value()},\n",
    "                      {\"Tokenizer\" : self.tokenizer}\n",
    "                  ]\n",
    "               }\n",
    "    \n",
    "    def get_name(self):\n",
    "        return super().get_name()\n",
    "    \n",
    "    def get_top_value(self):\n",
    "        return super().get_top_value()\n",
    "    \n",
    "    def get_sim_measure_min_threshold(self):\n",
    "        return super().get_sim_measure_min_threshold()\n",
    "    \n",
    "    def get_sim_matrix(self):\n",
    "        return super().get_sim_matrix()\n",
    "    \n",
    "    def get_tokenizer_type(self):\n",
    "        return type(self.tokenizer)\n",
    "    \n",
    "    def get_trace_links_df(self):\n",
    "        return super().get_trace_links_df()\n",
    "    \n",
    "    def save_sim_matrix(self):\n",
    "        super().save_sim_matrix()\n",
    "    \n",
    "    def get_model_dump_path(self):\n",
    "        return super().get_model_dump_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Recovering Efficiency\n",
    "\n",
    "In order to evaluate the efficiency of the algorithm tested (LSI), we use common metrics applied in the field of IR:\n",
    "\n",
    "    * Precision\n",
    "    * Recall\n",
    "    * F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis with Default Values of WordVec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Measures': {'Mean FScore of WordVec': 0.2714285714285714,\n",
      "              'Mean Precision of WordVec': 0.19047619047619047,\n",
      "              'Mean Recall of WordVec': 0.5},\n",
      " 'Setup': [{'Name': 'WordVec'},\n",
      "           {'Similarity Measure and Minimum Threshold': ('cosine', 0.8)},\n",
      "           {'Top Value': 3},\n",
      "           {'Tokenizer': <utils.tokenizers.WordNetBased_LemmaTokenizer object at 0x7f75d6575358>}]}\n"
     ]
    }
   ],
   "source": [
    "best_model = WordVec_BasedModel()\n",
    "best_model.recover_links(corpus, query, use_cases_names, bug_reports_names)\n",
    "evaluator = m_eval.ModelEvaluator(orc.oracle, best_model)\n",
    "evaluator.evaluate_model(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model hyperparameters search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:32, 10.59s/it]"
     ]
    }
   ],
   "source": [
    "all_hyperparams = {\n",
    "    WordVec_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value : [('cosine' ,x)  for x in [.75,.85,.95]],\n",
    "    WordVec_Model_Hyperp.TOP.value : [3,5],\n",
    "    WordVec_Model_Hyperp.TOKENIZER.value : [tok.PorterStemmerBased_Tokenizer(), tok.LancasterStemmerBased_Tokenizer(), \n",
    "                                                   tok.WordNetBased_LemmaTokenizer(), tok.SnowballStemmerBased_Tokenizer()]\n",
    "}\n",
    "\n",
    "hyperparams = aux_functions.generate_params_comb_list(**all_hyperparams)          \n",
    "\n",
    "print('Performing model hyperparameters search...')\n",
    "\n",
    "results_df = pd.DataFrame(columns=['precision', 'recall', 'fscore', 'model_name', 'top_value', 'tokenizer', 'metric', 'metric_value', 'model_dump', 'evaluator_dump'])\n",
    "\n",
    "#def run_model(idx, **hyperp):    \n",
    "for idx,hp in tqdm(enumerate(hyperparams)):\n",
    "    current_model = WordVec_BasedModel(**hp)\n",
    "    current_model.set_name('WordVec_Based_Model_{}'.format(idx))\n",
    "    current_model.recover_links(corpus, query, use_cases_names, bug_reports_names)\n",
    "    \n",
    "    evaluator = m_eval.ModelEvaluator(orc.oracle, current_model)\n",
    "    evaluator.evaluate_model()\n",
    "    evaluator.dump_model()\n",
    "    evaluator.dump_evaluator()\n",
    "    \n",
    "    results_df = results_df.append(pd.DataFrame([[evaluator.get_mean_precision(), \n",
    "                    evaluator.get_mean_recall(),\n",
    "                    evaluator.get_mean_fscore(), \n",
    "                    evaluator.get_model().get_name(),\n",
    "                    evaluator.get_model().get_top_value(),\n",
    "                    evaluator.get_model().get_tokenizer_type(),\n",
    "                    evaluator.get_model().get_sim_measure_min_threshold()[0],\n",
    "                    evaluator.get_model().get_sim_measure_min_threshold()[1],\n",
    "                    evaluator.get_model().get_model_dump_path(),\n",
    "                    evaluator.get_evaluator_dump_path()\n",
    "           ]], columns=results_df.columns), ignore_index=True)\n",
    "\n",
    "#tasks = [(idx,hp) for idx,hp in enumerate(hyperparams)]\n",
    "#results = Parallel(n_jobs=-1, verbose=1)(delayed(run_model)(idx, **hp) for idx,hp in tasks)\n",
    "results_df = results_df.astype(dtype={'model_dump' : str, 'evaluator_dump' : str, 'top_value': int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = aux_functions.report_best_model(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_sim_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model for TOP 3 and 5 - Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_functions.print_report_top_3_and_5_v3(results_df, metric=SimilairityMeasure.COSINE.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_functions.highlight_df(best_model.get_trace_links_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_functions.highlight_df(orc.oracle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
