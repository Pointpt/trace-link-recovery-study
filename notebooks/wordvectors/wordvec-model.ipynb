{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we demonstrate the use of **Word Embeddings (Word2Vec)** weighting technique into Information Retrieval to make trace link recovery between Use Cases and Bug Reports.\n",
    "\n",
    "We model our study as follows:\n",
    "\n",
    "* Each bug report title, summary and description compose a single query.\n",
    "* We use each use case content as an entire document that must be returned to the query made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support, pairwise_distances, pairwise\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import nltk\n",
    "import datetime\n",
    "import pprint\n",
    "from enum import Enum\n",
    "import pickle\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleLoader:\n",
    "    def __init__(self, rows_names, columns_names):\n",
    "        self.oracle = None\n",
    "        self._columns_names = columns_names\n",
    "        self._rows_names = rows_names\n",
    "    \n",
    "    def load(self):\n",
    "        self.oracle = pd.DataFrame(columns=list(self._columns_names), \n",
    "                                   data=np.zeros(shape=(len(self._rows_names), len(self._columns_names)), \n",
    "                                                 dtype='int64'))\n",
    "        self.oracle.insert(0, 'artf_name', list(self._rows_names))\n",
    "        \n",
    "        for index, row in trace_df.iterrows():\n",
    "            idx = self.oracle[self.oracle.artf_name == row['trg_artf']].index\n",
    "            self.oracle.at[idx, row['src_artf']] = row['link']\n",
    "\n",
    "        self.oracle.set_index('artf_name', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df = pd.read_csv('../../data/jEdit/jEditDataset/oracle/output/trace_matrix.csv')\n",
    "artfs_desc_df = pd.read_csv('../../data/jEdit/jEditDataset/oracle/output/artifacts_descriptions.csv', sep=\"|\")\n",
    "\n",
    "use_cases_df = artfs_desc_df[artfs_desc_df.artf_description.str.contains('Use Case ID')]\n",
    "bug_reports_df = artfs_desc_df[artfs_desc_df.artf_description.str.contains('Bug Number')]\n",
    "\n",
    "corpus = use_cases_df.artf_description\n",
    "query = bug_reports_df.artf_description\n",
    "\n",
    "use_cases_names = use_cases_df.artf_name\n",
    "bug_reports_names = bug_reports_df.artf_name\n",
    "\n",
    "orc = OracleLoader(use_cases_names, bug_reports_names)\n",
    "orc.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordVec Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVec_Model_Hyperp(Enum):\n",
    "    NAME = 'wordvec__name'\n",
    "    TOP = 'wordvec__top'\n",
    "    TOKENIZER = 'wordvec__tokenizer'\n",
    "    SIM_MEASURE_MIN_THRESHOLD = 'wordvec__sim_measure_min_threshold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Others stemmers are not relevant for our analysis:\n",
    " . RSLP Stemmer: portuguese language\n",
    " . ISRIS Stemmer: returns Arabic root for the given token \n",
    " . Regexp Stemmer: uses regulax expressions to identify morphological affixes\n",
    " \n",
    "Relevant Stemmers/Lemmatizers are implemented below. \n",
    "\"\"\"\n",
    "\n",
    "class GenericTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "    def __call__(self, doc):\n",
    "        tokens = [self.stemmer.stem(token) for token in nltk.word_tokenize(doc)]\n",
    "        #return [token.lower() for token in tokens if token.isalpha() and token not in self.stopwords and len(token) > 1]\n",
    "        return [token.lower() for token in tokens if token not in self.stopwords]\n",
    "        \n",
    "class WordNetBased_LemmaTokenizer(GenericTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = [self.wnl.lemmatize(token) for token in nltk.word_tokenize(doc)]\n",
    "        return [token.lower() for token in tokens if token.isalpha() and token not in self.stopwords]\n",
    "\n",
    "class LancasterStemmerBased_Tokenizer(GenericTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stemmer = nltk.stem.LancasterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return super().__call__(doc)\n",
    "\n",
    "class PorterStemmerBased_Tokenizer(GenericTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stemmer = nltk.stem.PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return super().__call__(doc)\n",
    "    \n",
    "class SnowballStemmerBased_Tokenizer(GenericTokenizer):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stemmer = nltk.stem.SnowballStemmer('english')    \n",
    "    def __call__(self, doc):\n",
    "        return super().__call__(doc)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Functions / Similarity Measures Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityMeasure(Enum):\n",
    "    COSINE = 'cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "params_dict = {\n",
    "    'wordvec__sim_measure_min_threshold' : ('cosine',.9),\n",
    "    'wordvec__name' : 'WordVec',\n",
    "    'wordvec__top' : 3\n",
    "    'wordvec_tokenizer' : WordNetBased_LemmaTokenizer()\n",
    "}\n",
    "\"\"\"\n",
    "class WordVec_BasedModel:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "            Each spaCy NLP model is a \n",
    "        \"\"\"\n",
    "        self._nlp_model = None\n",
    "        \n",
    "        self.name = None\n",
    "        self.sim_measure_min_threshold = None\n",
    "        self.top = None\n",
    "        self.trace_links_df = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        self.set_basic_params(**kwargs)\n",
    "        self.set_nlp_model()\n",
    "    \n",
    "    def set_name(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def set_basic_params(self, **kwargs):\n",
    "        self.name = 'WordVec' if WordVec_Model_Hyperp.NAME.value not in kwargs.keys() else kwargs[WordVec_Model_Hyperp.NAME.value]\n",
    "        self.sim_measure_min_threshold = (SimilarityMeasure.COSINE.value,.80) if WordVec_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value not in kwargs.keys() else kwargs[WordVec_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value]\n",
    "        self.top = 3 if WordVec_Model_Hyperp.TOP.value not in kwargs.keys() else kwargs[WordVec_Model_Hyperp.TOP.value]\n",
    "        self.tokenizer = WordNetBased_LemmaTokenizer() if WordVec_Model_Hyperp.TOKENIZER.value not in kwargs.keys() else kwargs[WordVec_Model_Hyperp.TOKENIZER.value]\n",
    "    \n",
    "    def set_nlp_model(self):\n",
    "        \"\"\"\n",
    "            WordVec based on GloVe 1.1M keys x 300 dim\n",
    "            300-dimensional word vectors trained on Common Crawl with GloVe.\n",
    "        \"\"\"\n",
    "        self._nlp_model = spacy.load('en_vectors_web_lg')\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        \"\"\"to pickle object serialization/deserialization\"\"\"\n",
    "        d = dict(self.__dict__)\n",
    "        del d['_nlp_model']\n",
    "        return d\n",
    "    \n",
    "    def __setstate__(self, d):\n",
    "        \"\"\"to pickle object serialization/deserialization\"\"\"\n",
    "        self.__dict__.update(d)\n",
    "    \n",
    "    def recover_links(self, corpus, query, use_cases_names, bug_reports_names):\n",
    "        return self._recover_links_cosine(corpus, query, use_cases_names, bug_reports_names)\n",
    "    \n",
    "    def _recover_links_cosine(self, corpus, query, use_cases_names, bug_reports_names):\n",
    "        list_corpus_tokens = [self.tokenizer.__call__(doc) for doc in corpus]\n",
    "        list_query_tokens = [self.tokenizer.__call__(doc) for doc in query]\n",
    "        \n",
    "        corpus = [' '.join(tok_list) for tok_list in list_corpus_tokens]\n",
    "        query = [' '.join(tok_list) for tok_list in list_query_tokens]\n",
    "        \n",
    "        self._sim_matrix = pd.DataFrame(index = use_cases_names, \n",
    "                                           columns = bug_reports_names,\n",
    "                                           data=np.zeros(shape=(len(use_cases_names), len(bug_reports_names)),dtype='float64'))\n",
    "        \n",
    "        for bug_id, bug_desc in zip(bug_reports_names, query):\n",
    "            for uc_id, uc_desc in zip(use_cases_names, corpus):\n",
    "                doc1 = self._nlp_model(bug_desc)\n",
    "                doc2 = self._nlp_model(uc_desc)\n",
    "                self._sim_matrix.at[uc_id, bug_id] = doc1.similarity(doc2)  # cosine similarity is default\n",
    "        \n",
    "        self._fillUp_traceLinksDf(use_cases_names, bug_reports_names, self._sim_matrix)        \n",
    "    \n",
    "    def _fillUp_traceLinksDf(self, use_cases_names, bug_reports_names, sim_matrix):\n",
    "        self.trace_links_df = pd.DataFrame(index = use_cases_names,\n",
    "                                           columns = bug_reports_names,\n",
    "                                           data = sim_matrix.values)\n",
    "        \n",
    "        for col in self.trace_links_df.columns:\n",
    "            nlargest_df = self.trace_links_df.nlargest(n = self.top, columns=col, keep='first')    \n",
    "            self.trace_links_df[col] = [1 if x in nlargest_df[col].tolist() and x >= self.sim_measure_min_threshold[1] else 0 for x in self.trace_links_df[col]]\n",
    "    \n",
    "    def model_setup(self):\n",
    "        return {\"Setup\" : \n",
    "                  [\n",
    "                      {\"Name\" : self.name},\n",
    "                      {\"Similarity Measure and Minimum Threshold\" : self.sim_measure_min_threshold},\n",
    "                      {\"Top Value\" : self.top},\n",
    "                      {\"Tokenizer\" : self.tokenizer}\n",
    "                  ]\n",
    "               }\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_top_value(self):\n",
    "        return self.top\n",
    "    \n",
    "    def get_sim_matrix(self):\n",
    "        return self._sim_matrix\n",
    "    \n",
    "    def get_sim_measure_min_threshold(self):\n",
    "        return self.sim_measure_min_threshold\n",
    "    \n",
    "    def get_trace_links_df(self):\n",
    "        return self.trace_links_df\n",
    "    \n",
    "    def get_tokenizer_type(self):\n",
    "        return type(self.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, oracle, model):\n",
    "        self.model = model\n",
    "        self.oracle = oracle\n",
    "        self.recovered_links = model.trace_links_df\n",
    "        \n",
    "        self.eval_df = pd.DataFrame(columns=['precision','recall','fscore','support'])\n",
    "        self.mean_precision = -1\n",
    "        self.mean_recall = -1\n",
    "        self.mean_fscore = -1\n",
    "    \n",
    "    def evaluate_model(self, verbose=False, file=None):\n",
    "        y_true = csr_matrix(self.oracle.values, dtype=int)\n",
    "        y_pred = csr_matrix(self.recovered_links.values, dtype=int)\n",
    "        \n",
    "        p, r, f, sp = precision_recall_fscore_support(y_true, y_pred)\n",
    "\n",
    "        i = 0\n",
    "        for idx, row in self.oracle.iteritems():\n",
    "            self.eval_df.at[idx, 'precision'] = p[i]\n",
    "            self.eval_df.at[idx, 'recall'] = r[i]\n",
    "            self.eval_df.at[idx, 'fscore'] = f[i]\n",
    "            self.eval_df.at[idx, 'support'] = sp[i]\n",
    "            i += 1\n",
    "        \n",
    "        self.mean_precision = self.eval_df.precision.mean()\n",
    "        self.mean_recall = self.eval_df.recall.mean()\n",
    "        self.mean_fscore = self.eval_df.fscore.mean()\n",
    "        \n",
    "        if verbose:\n",
    "            self.print_report(file)\n",
    "    \n",
    "    #def check_best_model(self, best_pre, best_rec, best_fs, best_md):\n",
    "    #    if best_rec <= self.get_mean_recall():\n",
    "    #        if best_pre <= self.get_mean_precision():\n",
    "    #            return (self.get_mean_precision(), self.get_mean_recall(), self.get_mean_fscore(), self.get_model())\n",
    "    #    return (best_pre, best_rec, best_fs, best_md)\n",
    "    \n",
    "    def print_report(self, file=None):\n",
    "        dic = self.model.model_setup()\n",
    "        dic['Measures'] = {}\n",
    "        dic['Measures']['Mean Precision of {}'.format(self.model.get_name())] = self.get_mean_precision()\n",
    "        dic['Measures']['Mean Recall of {}'.format(self.model.get_name())] = self.get_mean_recall()\n",
    "        dic['Measures']['Mean FScore of {}'.format(self.model.get_name())] = self.get_mean_fscore()\n",
    "        \n",
    "        if file is None:    \n",
    "            pprint.pprint(dic)\n",
    "        else:\n",
    "            file.write(pprint.pformat(dic))\n",
    "        \n",
    "    def plot_precision_vs_recall(self):\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.plot(self.eval_df.recall, self.eval_df.precision, 'ro', label='Precision vs Recall')\n",
    "\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlabel('Recall')\n",
    "\n",
    "        plt.axis([0, 1.1, 0, 1.1])\n",
    "        plt.title(\"Precision vs Recall Plot - \" + self.model.get_name())\n",
    "        plt.show()\n",
    "    \n",
    "    def save_log(self):\n",
    "        print(\"\\nSaving model log...\")\n",
    "        with open('../logs/' + str(datetime.datetime.now()) + '.txt', 'a') as f:\n",
    "            evaluator.evaluate_model(verbose=True, file=f)\n",
    "        print(\"Model log saved with success!\")\n",
    "            \n",
    "    def get_mean_precision(self):\n",
    "        return self.mean_precision\n",
    "    \n",
    "    def get_mean_recall(self):\n",
    "        return self.mean_recall\n",
    "    \n",
    "    def get_mean_fscore(self):\n",
    "        return self.mean_fscore\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Recovering Efficiency\n",
    "\n",
    "In order to evaluate the efficiency of the algorithm tested (LSI), we use common metrics applied in the field of IR:\n",
    "\n",
    "    * Precision\n",
    "    * Recall\n",
    "    * F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis with Default Values of LSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Measures': {'Mean FScore of WordVec': 0.2714285714285714,\n",
      "              'Mean Precision of WordVec': 0.19047619047619047,\n",
      "              'Mean Recall of WordVec': 0.5},\n",
      " 'Setup': [{'Name': 'WordVec'},\n",
      "           {'Similarity Measure and Minimum Threshold': ('cosine', 0.8)},\n",
      "           {'Top Value': 3},\n",
      "           {'Tokenizer': <__main__.WordNetBased_LemmaTokenizer object at 0x7f5c8c30e4a8>}]}\n"
     ]
    }
   ],
   "source": [
    "best_model = WordVec_BasedModel()\n",
    "best_model.recover_links(corpus, query, use_cases_names, bug_reports_names)\n",
    "evaluator = ModelEvaluator(orc.oracle, best_model)\n",
    "evaluator.evaluate_model(verbose=True)\n",
    "#evaluator.plot_precision_vs_recall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def generate_params_comb_list(**kwargs):\n",
    "    list_params = []\n",
    "    for key, values in kwargs.items():\n",
    "        aux_list = []\n",
    "        for v in values:\n",
    "            aux_list.append((key, v))\n",
    "        list_params.append(aux_list)\n",
    "    \n",
    "    list_tuples = list(product(*list_params))\n",
    "    \n",
    "    list_dicts = []\n",
    "    for ex_tup in list_tuples:\n",
    "        dic = {}\n",
    "        for in_tup in ex_tup:\n",
    "            dic[in_tup[0]] = in_tup[1]\n",
    "        list_dicts.append(dic)\n",
    "        \n",
    "    return list_dicts\n",
    "\n",
    "\n",
    "def plot_heatmap(results_df):\n",
    "    tmp_df = pd.DataFrame({'precision': results_df['precision'], \n",
    "                           'recall' : results_df['recall'], \n",
    "                           'fscore': results_df['fscore'], \n",
    "                           'model': results_df['model_name']})\n",
    "    tmp_df.set_index('model', inplace=True)\n",
    "    fig, ax = plt.subplots(figsize=(10, 20)) \n",
    "    ax = sns.heatmap(tmp_df, vmin=0, vmax=1, linewidths=.5, cmap=\"Greens\", annot=True, cbar=False, ax=ax)\n",
    "\n",
    "\n",
    "def highlight_df(df):\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    return df.style.background_gradient(cmap=cm)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model hyperparameters search...\n"
     ]
    }
   ],
   "source": [
    "all_hyperparams = {\n",
    "    WordVec_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value : [('cosine' ,x)  for x in [.75,.85,.95]],\n",
    "    WordVec_Model_Hyperp.TOP.value : [3,5],\n",
    "    WordVec_Model_Hyperp.TOKENIZER.value : [PorterStemmerBased_Tokenizer(), LancasterStemmerBased_Tokenizer(), \n",
    "                                                   WordNetBased_LemmaTokenizer(), SnowballStemmerBased_Tokenizer()]\n",
    "}\n",
    "\n",
    "hyperparams = generate_params_comb_list(**all_hyperparams)          \n",
    "\n",
    "print('Performing model hyperparameters search...')\n",
    "\n",
    "results_df = pd.DataFrame(columns=['precision', 'recall', 'fscore', 'model_name', 'top_value', 'tokenizer', 'metric', 'metric_value', 'model_dump', 'evaluator_dump'])\n",
    "\n",
    "#def run_model(idx, **hyperp):    \n",
    "for idx,hp in enumerate(hyperparams):\n",
    "    current_model = WordVec_BasedModel(**hp)\n",
    "    current_model.set_name('WordVec_Based_Model_{}'.format(idx))\n",
    "    current_model.recover_links(corpus, query, use_cases_names, bug_reports_names)\n",
    "    \n",
    "    evaluator = ModelEvaluator(orc.oracle, current_model)\n",
    "    evaluator.evaluate_model()\n",
    "    \n",
    "    model_dump = 'dumps/model/{}.p'.format(evaluator.get_model().get_name())\n",
    "    evaluator_dump = 'dumps/evaluator/eval_{}.p'.format(evaluator.get_model().get_name())\n",
    "    \n",
    "    pickle.dump(evaluator.get_model(), open(model_dump, 'wb'))\n",
    "    pickle.dump(evaluator, open(evaluator_dump, 'wb'))\n",
    "    \n",
    "    results_df.append(pd.DataFrame([[evaluator.get_mean_precision(), \n",
    "                    evaluator.get_mean_recall(),\n",
    "                    evaluator.get_mean_fscore(), \n",
    "                    evaluator.get_model().get_name(),\n",
    "                    evaluator.get_model().get_top_value(),\n",
    "                    evaluator.get_model().get_tokenizer_type(),\n",
    "                    evaluator.get_model().get_sim_measure_min_threshold()[0],\n",
    "                    evaluator.get_model().get_sim_measure_min_threshold()[1],\n",
    "                    model_dump,\n",
    "                    evaluator_dump\n",
    "           ]], columns=results_df.columns))\n",
    "\n",
    "#tasks = [(idx,hp) for idx,hp in enumerate(hyperparams)]\n",
    "#results = Parallel(n_jobs=-1, verbose=1)(delayed(run_model)(idx, **hp) for idx,hp in tasks)\n",
    "results_df = results_df.astype(dtype={'model_dump' : str, 'evaluator_dump' : str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "      <th>model_name</th>\n",
       "      <th>top_value</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>metric</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model_dump</th>\n",
       "      <th>evaluator_dump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [precision, recall, fscore, model_name, top_value, tokenizer, metric, metric_value, model_dump, evaluator_dump]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Report -------------------\n",
      "\n",
      "Total of Analyzed Hyperparameters Combinations: 24\n",
      "\n",
      "Best Model Hyperparameters Combination Found:\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bb4a6286e841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBest Model Hyperparameters Combination Found:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrow_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_dump'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_dump'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mevalu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evaluator_dump'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trace-link-recovery-study/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "print(\"------------ Report -------------------\\n\")\n",
    "print(\"Total of Analyzed Hyperparameters Combinations: {}\".format(len(hyperparams)))\n",
    "\n",
    "print(\"\\nBest Model Hyperparameters Combination Found:\\n\")            \n",
    "\n",
    "row_idx = results_df['model_dump'][results_df.recall == results_df.recall.max()].index[0]\n",
    "best_model = pickle.load(open(results_df['model_dump'][row_idx], 'rb'))\n",
    "evalu = pickle.load(open(results_df['evaluator_dump'][row_idx], 'rb'))\n",
    "evalu.evaluate_model(verbose=True)\n",
    "\n",
    "#print(\"\\nPlot Precision vs Recall - Best Model\")\n",
    "#evalu.plot_precision_vs_recall()\n",
    "\n",
    "#print(\"\\nHeatmap of All Models\")\n",
    "#plot_heatmap(results_df)\n",
    "\n",
    "#evalu.save_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model for TOP 3 and 5 - Cosine 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top in [3,5]:\n",
    "    row_idx_top = results_df[(results_df.top_value == top) & (results_df.metric == SimilarityMeasure.COSINE.value)].recall.argmax()\n",
    "    best_model_top = pickle.load(open(results_df['model_dump'][row_idx_top], 'rb'))\n",
    "    evalu_top = pickle.load(open(results_df['evaluator_dump'][row_idx_top], 'rb'))\n",
    "    evalu_top.evaluate_model(verbose=True)\n",
    "    print(\"------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_df(best_model.get_trace_links_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_df(orc.oracle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
