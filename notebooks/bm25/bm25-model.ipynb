{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we demonstrate the use of **BM25 (Best Matching 25)** Information Retrieval technique to make trace link recovery between Use Cases and Bug Reports.\n",
    "\n",
    "We model our study as follows:\n",
    "\n",
    "* Each bug report title, summary and description compose a single query.\n",
    "* We use each use case content as an entire document that must be returned to the query made\n",
    "\n",
    "This code is based on implementation of [Alessandra Sozzi](https://github.com/AlessandraSozzi/Lucene-python/blob/master/LuceneScoringFunctions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from gensim.summarization.bm25 import BM25\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, pairwise\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import datetime\n",
    "import pprint\n",
    "from enum import Enum\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df = pd.read_csv('../../data/jEdit/jEditDataset/oracle/output/trace_matrix.csv')\n",
    "artfs_desc_df = pd.read_csv('../../data/jEdit/jEditDataset/oracle/output/artifacts_descriptions.csv', sep=\"|\")\n",
    "\n",
    "use_cases_df = artfs_desc_df[artfs_desc_df.artf_description.str.contains('Use Case ID')]\n",
    "bug_reports_df = artfs_desc_df[artfs_desc_df.artf_description.str.contains('Bug Number')]\n",
    "\n",
    "corpus = use_cases_df.artf_description\n",
    "query = bug_reports_df.artf_description\n",
    "\n",
    "use_cases_names = use_cases_df.artf_name\n",
    "bug_reports_names = bug_reports_df.artf_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25_Model_Hyperp(Enum):\n",
    "    NAME = 'bm25__name'\n",
    "    K = 'bm25__k'\n",
    "    B = 'bm25__b'\n",
    "    VECTORIZER = 'bm25__vectorizer'\n",
    "    VECTORIZER_STOP_WORDS = 'bm25__vectorizer__stop_words'\n",
    "    VECTORIZER_TOKENIZER = 'bm25__vectorizer__tokenizer'\n",
    "    VECTORIZER_USE_IDF = 'bm25__vectorizer__use_idf'\n",
    "    VECTORIZER_SMOOTH_IDF = 'bm25__vectorizer__smooth_idf'\n",
    "    VECTORIZER_NGRAM_RANGE = 'bm25__vectorizer__ngram_range'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Others stemmers are not relevant for our analysis:\n",
    " . RSLP Stemmer: portuguese language\n",
    " . ISRIS Stemmer: returns Arabic root for the given token \n",
    " . Regexp Stemmer: uses regulax expressions to identify morphological affixes\n",
    " \n",
    "Relevant Stemmers/Lemmatizers are implemented below. \n",
    "\"\"\"\n",
    "\n",
    "class WordNetBased_LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(token) for token in word_tokenize(doc)]\n",
    "\n",
    "class LancasterStemmerBased_Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = LancasterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(token) for token in word_tokenize(doc)]\n",
    "\n",
    "class PorterStemmerBased_Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(token) for token in word_tokenize(doc)]\n",
    "    \n",
    "class SnowballStemmerBased_Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(token) for token in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_1: [0.6192874727082551, 0, 0, 0]\n",
      "result_2: [0.0, 0.0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.bm25 import BM25\n",
    "\n",
    "corpus__ = [\n",
    "     [\"black\", \"cat\", \"white\", \"cat\", \"blacks\"],\n",
    "     [\"cat\", \"outer\", \"space\"],\n",
    "     [\"wag\", \"dog\"],\n",
    "     ['blacks']\n",
    " ]\n",
    "\n",
    "queries = [['black'], ['cat']]\n",
    "\n",
    "bm25__ = BM25(corpus__)\n",
    "\n",
    "average_idf = sum(map(lambda k: float(bm25__.idf[k]), bm25__.idf.keys())) / len(bm25__.idf.keys())\n",
    "\n",
    "result_1 = bm25__.get_scores(queries[0], average_idf=average_idf)\n",
    "result_2 = bm25__.get_scores(queries[1], average_idf=average_idf)\n",
    "\n",
    "print('result_1: {}'.format(result_1))\n",
    "print('result_2: {}'.format(result_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "params_dict = {\n",
    "    'bm25__k' : 1.2,\n",
    "    'bm25__b' : 0.75,\n",
    "    'bm25__name' : 'BM25',\n",
    "    'bm25__vectorizer__stop_words' : 'english',\n",
    "    'bm25__vectorizer__tokenizer' : Tokenizer(),\n",
    "    'bm25__vectorizer__use_idf' : True,          # optional if type(Vectorizer) == TfidfVectorizer\n",
    "    'bm25__vectorizer__smooth_idf' : True,       # optional if type(Vectorizer) == TfidfVectorizer\n",
    "    'bm25__vectorizer__ngram_range' : (1,2)\n",
    "}\n",
    "\"\"\"\n",
    "class BM25_Customized:\n",
    "    # k = 1.2, b = 0.75, coord_factor = False\n",
    "    def __init__(self, **kwargs):\n",
    "        self.k = None\n",
    "        self.b = None\n",
    "        self.name = None\n",
    "        #self.vectorizer = None    # NOT USED BY NOW!!!!\n",
    "        self.trace_links_df = None\n",
    "        \n",
    "        self.set_params(**kwargs)\n",
    "        #self.set_vectorizer(**kwargs)\n",
    "\n",
    "    \n",
    "    def recover_links(self, corpus, query, use_cases_names, bug_reports_names):\n",
    "        bm25 = BM25(corpus)\n",
    "        average_idf = sum(map(lambda k: float(bm25.idf[k]), bm25.idf.keys())) / len(bm25.idf.keys())\n",
    "        \n",
    "        self.trace_links_df = pd.DataFrame(index = use_cases_names, \n",
    "                                           columns = bug_reports_names,\n",
    "                                           data=np.zeros(shape=(len(use_cases_names), len(bug_reports_names)),dtype='int8'))\n",
    "               \n",
    "        for bug_id, bug_desc in zip(bug_reports_names, query):\n",
    "            scores = bm25.get_scores(bug_desc, average_idf=average_idf)\n",
    "            print('bug_id: {} \\nscores: {}'.format(bug_id, scores))\n",
    "            for idx,score in enumerate(scores):\n",
    "                #print('idx: {}\\nuse_case_names[idx]: {}\\nscore: {}'.format(idx, use_cases_names.tolist()[idx], score))\n",
    "                self.trace_links_df.at[use_cases_names.tolist()[idx], bug_id] = score\n",
    "        \n",
    "        #for col in self.trace_links_df.columns:\n",
    "        #    self.trace_links_df[col] = [1 if x >= self.sim_measure_min_threshold[1] else 0 for x in self.trace_links_df[col]]\n",
    "    \n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        self.name = 'BM25' if BM25_Model_Hyperp.NAME.value not in kwargs.keys() else kwargs[BM25_Model_Hyperp.NAME.value]\n",
    "        self.k = 1.2 if BM25_Model_Hyperp.K.value not in kwargs.keys() else kwargs[BM25_Model_Hyperp.K.value]\n",
    "        self.b = 0.75 if BM25_Model_Hyperp.B.value not in kwargs.keys() else kwargs[BM25_Model_Hyperp.B.value]\n",
    "        \n",
    "    #def set_vectorizer(self, **kwargs):\n",
    "    #    self.vectorizer = TfidfVectorizer(stop_words='english',\n",
    "    #                                         use_idf=True, \n",
    "    #                                         smooth_idf=True) if BM25_Model_Hyperp.VECTORIZER.value not in kwargs.keys() else kwargs[BM25_Model_Hyperp.VECTORIZER.value]\n",
    "    #    \n",
    "    #    vec_params = {key.split('__')[2]:kwargs[key] for key,val in kwargs.items() if '__vectorizer__' in key}\n",
    "    #    self.vectorizer.set_params(**vec_params)\n",
    "    \n",
    "    \n",
    "    def model_setup(self):\n",
    "        return {\"Setup\" : \n",
    "                  [\n",
    "                      {\"K\" : self.k},\n",
    "                      {\"B\" : self.b},\n",
    "                      {\"Vectorizer\" : self.vectorizer.get_params()},\n",
    "                      {\"Vectorizer Type\" : type(self.vectorizer)}\n",
    "                  ]\n",
    "               }\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_trace_links_df(self):\n",
    "        return self.trace_links_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleLoader:\n",
    "    def __init__(self, rows_names, columns_names):\n",
    "        self.oracle = None\n",
    "        self._columns_names = columns_names\n",
    "        self._rows_names = rows_names\n",
    "    \n",
    "    def load(self):\n",
    "        self.oracle = pd.DataFrame(columns=list(self._columns_names))\n",
    "        self.oracle.insert(0, 'artf_name', list(self._rows_names))\n",
    "        \n",
    "        for index, row in trace_df.iterrows():\n",
    "            idx = self.oracle[self.oracle.artf_name == row['trg_artf']].index\n",
    "            self.oracle.at[idx, row['src_artf']] = row['link']\n",
    "\n",
    "        self.oracle.set_index('artf_name', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, oracle, model):\n",
    "        self.model = model\n",
    "        self.oracle = oracle\n",
    "        self.recovered_links = model.trace_links_df\n",
    "        \n",
    "        self.eval_df = pd.DataFrame(columns=['precision','recall','fscore','support'])\n",
    "        self.mean_precision = -1\n",
    "        self.mean_recall = -1\n",
    "        self.mean_fscore = -1\n",
    "    \n",
    "    def evaluate_model(self, verbose=False, file=None):\n",
    "        y_true = csr_matrix(self.oracle.values, dtype=int)\n",
    "        y_pred = csr_matrix(self.recovered_links.values, dtype=int)\n",
    "        \n",
    "        p, r, f, sp = precision_recall_fscore_support(y_true, y_pred)\n",
    "\n",
    "        i = 0\n",
    "        for idx, row in self.oracle.iteritems():\n",
    "            self.eval_df.at[idx, 'precision'] = p[i]\n",
    "            self.eval_df.at[idx, 'recall'] = r[i]\n",
    "            self.eval_df.at[idx, 'fscore'] = f[i]\n",
    "            self.eval_df.at[idx, 'support'] = sp[i]\n",
    "            i += 1\n",
    "        \n",
    "        self.mean_precision = self.eval_df.precision.mean()\n",
    "        self.mean_recall = self.eval_df.recall.mean()\n",
    "        self.mean_fscore = self.eval_df.fscore.mean()\n",
    "        \n",
    "        if verbose:\n",
    "            self.print_report(file)\n",
    "    \n",
    "    def print_report(self, file=None):\n",
    "        dic = self.model.model_setup()\n",
    "        dic['Measures'] = {}\n",
    "        dic['Measures']['Mean Precision of {}'.format(self.model.get_name())] = self.get_mean_precision()\n",
    "        dic['Measures']['Mean Recall of {}'.format(self.model.get_name())] = self.get_mean_recall()\n",
    "        dic['Measures']['Mean FScore of {}'.format(self.model.get_name())] = self.get_mean_fscore()\n",
    "        \n",
    "        if file is None:    \n",
    "            pprint.pprint(dic)\n",
    "        else:\n",
    "            file.write(pprint.pformat(dic))\n",
    "        \n",
    "    def plot_precision_vs_recall(self):\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.plot(self.eval_df.recall, self.eval_df.precision, 'ro', label='Precision vs Recall')\n",
    "\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlabel('Recall')\n",
    "\n",
    "        plt.axis([0, 1.1, 0, 1.1])\n",
    "        plt.title(\"Precision vs Recall Plot - \" + self.model.get_name())\n",
    "        plt.show()\n",
    "    \n",
    "    def save_log(self):\n",
    "        print(\"\\nSaving model log...\")\n",
    "        with open('../logs/' + str(datetime.datetime.now()) + '.txt', 'a') as f:\n",
    "            evaluator.evaluate_model(verbose=True, file=f)\n",
    "        print(\"Model log saved with success!\")\n",
    "            \n",
    "    def get_mean_precision(self):\n",
    "        return self.mean_precision\n",
    "    \n",
    "    def get_mean_recall(self):\n",
    "        return self.mean_recall\n",
    "    \n",
    "    def get_mean_fscore(self):\n",
    "        return self.mean_fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Recovering Efficiency\n",
    "\n",
    "In order to evaluate the efficiency of the algorithm tested (LSI), we use common metrics applied in the field of IR:\n",
    "\n",
    "    * Precision\n",
    "    * Recall\n",
    "    * F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orc = OracleLoader(use_cases_names, bug_reports_names)\n",
    "orc.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Generate Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def generate_params_comb_list(**kwargs):\n",
    "    list_params = []\n",
    "    for key, values in kwargs.items():\n",
    "        aux_list = []\n",
    "        for v in values:\n",
    "            aux_list.append((key, v))\n",
    "        list_params.append(aux_list)\n",
    "    \n",
    "    list_tuples = list(product(*list_params))\n",
    "    \n",
    "    list_dicts = []\n",
    "    for ex_tup in list_tuples:\n",
    "        dic = {}\n",
    "        for in_tup in ex_tup:\n",
    "            dic[in_tup[0]] = in_tup[1]\n",
    "        list_dicts.append(dic)\n",
    "        \n",
    "    return list_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(results_dict):\n",
    "    fig, ax = plt.subplots(figsize=(10,100)) \n",
    "    df = pd.DataFrame(results_dict)\n",
    "    df.set_index('model', inplace=True)       \n",
    "    ax = sns.heatmap(df, vmin=0, vmax=1, linewidths=.5, cmap=\"Greens\", annot=True, cbar=False, ax=ax)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hyperparams = {\n",
    "    LSI_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value : [('cosine',.75), ('cosine',.80), ('cosine',.85), ('cosine',.90), ('cosine',.95)],\n",
    "    LSI_Model_Hyperp.SVD_MODEL_N_COMPONENTS.value: [5,10,20,50,100],\n",
    "    LSI_Model_Hyperp.VECTORIZER_NGRAM_RANGE.value: [(1,1), (1,2)],\n",
    "    LSI_Model_Hyperp.VECTORIZER.value : [TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True), \n",
    "                         CountVectorizer(stop_words='english')],\n",
    "    LSI_Model_Hyperp.VECTORIZER_TOKENIZER.value : [PorterStemmerBased_Tokenizer(), LancasterStemmerBased_Tokenizer(), \n",
    "                                                   WordNetBased_LemmaTokenizer(), SnowballStemmerBased_Tokenizer()]\n",
    "}\n",
    "\n",
    "hyperparams = generate_params_comb_list(**all_hyperparams)\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "best_precision = -1\n",
    "best_recall = -1\n",
    "best_fscore = -1\n",
    "best_model = None\n",
    "\n",
    "results = {'precision': [], 'recall': [], 'fscore': [], 'model': []}\n",
    "\n",
    "i = 0\n",
    "for hyperp in hyperparams:\n",
    "    hyperp[LSI_Model_Hyperp.NAME.value] = 'LSI_Model_{}'.format(i)\n",
    "    current_model = LSI(**hyperp)\n",
    "    current_model.recover_links(corpus, query, use_cases_names, bug_reports_names)\n",
    "    \n",
    "    evaluator = ModelEvaluator(orc.oracle, current_model)\n",
    "    evaluator.evaluate_model()\n",
    "\n",
    "    if best_recall <= evaluator.get_mean_recall():\n",
    "        if best_precision <= evaluator.get_mean_precision():\n",
    "            best_recall = evaluator.get_mean_recall()\n",
    "            best_precision = evaluator.get_mean_precision()\n",
    "            best_fscore = evaluator.get_mean_fscore()\n",
    "            best_model = current_model\n",
    "    \n",
    "    results['precision'].append(evaluator.get_mean_precision())\n",
    "    results['recall'].append(evaluator.get_mean_recall())\n",
    "    results['fscore'].append(evaluator.get_mean_fscore())\n",
    "    results['model'].append(current_model.get_name())\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"------------ Report -------------------\\n\")\n",
    "print(\"Total of Analyzed Hyperparameters Combinations: {}\".format(len(hyperparams)))\n",
    "\n",
    "print(\"\\nBest Model and Hyperparameters Found: {}\\n\".format(best_model.get_name()))            \n",
    "evaluator = ModelEvaluator(orc.oracle, best_model)\n",
    "evaluator.evaluate_model(verbose=True)\n",
    "\n",
    "print(\"\\nPlot Precision vs Recall - Best Model\")\n",
    "evaluator.plot_precision_vs_recall()\n",
    "\n",
    "print(\"\\nHeatmap of All Models\")\n",
    "plot_heatmap(results)\n",
    "\n",
    "#evaluator.save_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hyperparams_2 = {\n",
    "    LSI_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value : [('cosine', x) for x in np.arange(1,100, .1)],\n",
    "    LSI_Model_Hyperp.SVD_MODEL_N_COMPONENTS.value: [best_model.svd_model.n_components],\n",
    "    LSI_Model_Hyperp.VECTORIZER_NGRAM_RANGE.value: [best_model.vectorizer.ngram_range],\n",
    "    LSI_Model_Hyperp.VECTORIZER.value : [best_model.vectorizer],\n",
    "    LSI_Model_Hyperp.VECTORIZER_TOKENIZER.value : [best_model.vectorizer.tokenizer]\n",
    "}\n",
    "\n",
    "hyperparams = generate_params_comb_list(**all_hyperparams)\n",
    "\n",
    "dic = {'precision':[], 'recall':[], 'threshold':[]}\n",
    "\n",
    "for hyperp in hyperparams:\n",
    "    current_model = LSI(**hyperp)\n",
    "    current_model.recover_links(corpus, query, use_cases_names, bug_reports_names)\n",
    "    \n",
    "    evaluator = ModelEvaluator(orc.oracle, current_model)\n",
    "    evaluator.evaluate_model()\n",
    "    \n",
    "    dic['precision'].append(evaluator.get_mean_precision())\n",
    "    dic['recall'].append(evaluator.get_mean_recall())\n",
    "    dic['threshold'].append(current_model.get_sim_measure_min_threshold()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dic)\n",
    "\n",
    "plt.plot(df['threshold'], df['precision'], 'r-')\n",
    "plt.plot(df['threshold'], df['recall'], 'b-')\n",
    "plt.xlabel('threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis with Default Values of BM25 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bug_id: BR_4020_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_3890_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_3844_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_4065_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_3880_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_3987_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_4067_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_3973_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_3898_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_3908_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_4058_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_4018_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_4005_SRC \n",
      "scores: [0, 0, 0]\n",
      "bug_id: BR_3974_SRC \n",
      "scores: [0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>artf_name</th>\n",
       "      <th>BR_4020_SRC</th>\n",
       "      <th>BR_3890_SRC</th>\n",
       "      <th>BR_3844_SRC</th>\n",
       "      <th>BR_4065_SRC</th>\n",
       "      <th>BR_3880_SRC</th>\n",
       "      <th>BR_3987_SRC</th>\n",
       "      <th>BR_4067_SRC</th>\n",
       "      <th>BR_3973_SRC</th>\n",
       "      <th>BR_3898_SRC</th>\n",
       "      <th>BR_3908_SRC</th>\n",
       "      <th>BR_4058_SRC</th>\n",
       "      <th>BR_4018_SRC</th>\n",
       "      <th>BR_4005_SRC</th>\n",
       "      <th>BR_3974_SRC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artf_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UC_003_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC_007_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC_010_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC_002_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC_006_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC_004_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC_005_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC_008_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC_001_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC_009_TRG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "artf_name   BR_4020_SRC  BR_3890_SRC  BR_3844_SRC  BR_4065_SRC  BR_3880_SRC  \\\n",
       "artf_name                                                                     \n",
       "UC_003_TRG            0            0            0            0            0   \n",
       "UC_007_TRG            0            0            0            0            0   \n",
       "UC_010_TRG            0            0            0            0            0   \n",
       "UC_002_TRG            0            0            0            0            0   \n",
       "UC_006_TRG            0            0            0            0            0   \n",
       "UC_004_TRG            0            0            0            0            0   \n",
       "UC_005_TRG            0            0            0            0            0   \n",
       "UC_008_TRG            0            0            0            0            0   \n",
       "UC_001_TRG            0            0            0            0            0   \n",
       "UC_009_TRG            0            0            0            0            0   \n",
       "\n",
       "artf_name   BR_3987_SRC  BR_4067_SRC  BR_3973_SRC  BR_3898_SRC  BR_3908_SRC  \\\n",
       "artf_name                                                                     \n",
       "UC_003_TRG            0            0            0            0            0   \n",
       "UC_007_TRG            0            0            0            0            0   \n",
       "UC_010_TRG            0            0            0            0            0   \n",
       "UC_002_TRG            0            0            0            0            0   \n",
       "UC_006_TRG            0            0            0            0            0   \n",
       "UC_004_TRG            0            0            0            0            0   \n",
       "UC_005_TRG            0            0            0            0            0   \n",
       "UC_008_TRG            0            0            0            0            0   \n",
       "UC_001_TRG            0            0            0            0            0   \n",
       "UC_009_TRG            0            0            0            0            0   \n",
       "\n",
       "artf_name   BR_4058_SRC  BR_4018_SRC  BR_4005_SRC  BR_3974_SRC  \n",
       "artf_name                                                       \n",
       "UC_003_TRG            0            0            0            0  \n",
       "UC_007_TRG            0            0            0            0  \n",
       "UC_010_TRG            0            0            0            0  \n",
       "UC_002_TRG            0            0            0            0  \n",
       "UC_006_TRG            0            0            0            0  \n",
       "UC_004_TRG            0            0            0            0  \n",
       "UC_005_TRG            0            0            0            0  \n",
       "UC_008_TRG            0            0            0            0  \n",
       "UC_001_TRG            0            0            0            0  \n",
       "UC_009_TRG            0            0            0            0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = BM25_Customized()\n",
    "best_model.recover_links(corpus, query, use_cases_names, bug_reports_names)\n",
    "\n",
    "best_model.get_trace_links_df()\n",
    "\n",
    "#evaluator = ModelEvaluator(orc.oracle, best_model)\n",
    "#evaluator.evaluate_model(verbose=True)\n",
    "#evaluator.plot_precision_vs_recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_1 = pd.DataFrame(np.matrix(best_model.trace_links_df))\n",
    "sims_2 = pd.DataFrame(np.matrix(pairwise.cosine_similarity(best_model.get_svd_matrix(), best_model.get_query_vector()) ))\n",
    "\n",
    "for col in sims_2.columns:\n",
    "    sims_2[col] = [1 if x >= best_model.get_sim_measure_min_threshold()[1] else 0 for x in sims_2[col]]\n",
    "\n",
    "sims_1.equals(sims_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
